{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the QLattice with small data sets\n",
    "\n",
    "In many cases, a researcher has collected a fairly small dataset of a few hundred individuals. In this notebook we apply a systematic approach to test the performance of the QLattice against the usual go-to technologies for fitting and ML.\n",
    "\n",
    "We compare to other **interpretable** models:\n",
    "- Linear models (both with and without LASSO)\n",
    "- Decision Trees\n",
    "\n",
    "And to other **ensemble models** which are more black-box:\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmlb\n",
    "import pandas as pd\n",
    "import feyn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n"
     ]
    }
   ],
   "source": [
    "print(len(pmlb.classification_dataset_names)+len(pmlb.regression_dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pmlb.regression_dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in pmlb.regression_dataset_names:\n",
    "#    df = pmlb.fetch_data(name, local_cache_dir=\"/tmp/pmlb_data\")\n",
    "#    print(f\"('{name}', {len(df)}, {len(df.columns)}),\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.DataFrame([\n",
    "('1027_ESL', 488, 5),\n",
    "('1028_SWD', 1000, 11),\n",
    "('1029_LEV', 1000, 5),\n",
    "('1030_ERA', 1000, 5),\n",
    "('1089_USCrime', 47, 14),\n",
    "('1096_FacultySalaries', 50, 5),\n",
    "('1191_BNG_pbc', 1000000, 19),\n",
    "('1193_BNG_lowbwt', 31104, 10),\n",
    "('1196_BNG_pharynx', 1000000, 11),\n",
    "('1199_BNG_echoMonths', 17496, 10),\n",
    "('1201_BNG_breastTumor', 116640, 10),\n",
    "('1203_BNG_pwLinear', 177147, 11),\n",
    "('1595_poker', 1025010, 11),\n",
    "('192_vineyard', 52, 3),\n",
    "('195_auto_price', 159, 16),\n",
    "('197_cpu_act', 8192, 22),\n",
    "('201_pol', 15000, 49),\n",
    "('207_autoPrice', 159, 16),\n",
    "('210_cloud', 108, 6),\n",
    "('215_2dplanes', 40768, 11),\n",
    "('218_house_8L', 22784, 9),\n",
    "('225_puma8NH', 8192, 9),\n",
    "('227_cpu_small', 8192, 13),\n",
    "('228_elusage', 55, 3),\n",
    "('229_pwLinear', 200, 11),\n",
    "('230_machine_cpu', 209, 7),\n",
    "('294_satellite_image', 6435, 37),\n",
    "('344_mv', 40768, 11),\n",
    "('4544_GeographicalOriginalofMusic', 1059, 118),\n",
    "('485_analcatdata_vehicle', 48, 5),\n",
    "('503_wind', 6574, 15),\n",
    "('505_tecator', 240, 125),\n",
    "('519_vinnie', 380, 3),\n",
    "('522_pm10', 500, 8),\n",
    "('523_analcatdata_neavote', 100, 3),\n",
    "('527_analcatdata_election2000', 67, 15),\n",
    "('529_pollen', 3848, 5),\n",
    "('537_houses', 20640, 9),\n",
    "('542_pollution', 60, 16),\n",
    "('547_no2', 500, 8),\n",
    "('556_analcatdata_apnea2', 475, 4),\n",
    "('557_analcatdata_apnea1', 475, 4),\n",
    "('560_bodyfat', 252, 15),\n",
    "('561_cpu', 209, 8),\n",
    "('562_cpu_small', 8192, 13),\n",
    "('564_fried', 40768, 11),\n",
    "('573_cpu_act', 8192, 22),\n",
    "('574_house_16H', 22784, 17),\n",
    "('579_fri_c0_250_5', 250, 6),\n",
    "('581_fri_c3_500_25', 500, 26),\n",
    "('582_fri_c1_500_25', 500, 26),\n",
    "('583_fri_c1_1000_50', 1000, 51),\n",
    "('584_fri_c4_500_25', 500, 26),\n",
    "('586_fri_c3_1000_25', 1000, 26),\n",
    "('588_fri_c4_1000_100', 1000, 101),\n",
    "('589_fri_c2_1000_25', 1000, 26),\n",
    "('590_fri_c0_1000_50', 1000, 51),\n",
    "('591_fri_c1_100_10', 100, 11),\n",
    "('592_fri_c4_1000_25', 1000, 26),\n",
    "('593_fri_c1_1000_10', 1000, 11),\n",
    "('594_fri_c2_100_5', 100, 6),\n",
    "('595_fri_c0_1000_10', 1000, 11),\n",
    "('596_fri_c2_250_5', 250, 6),\n",
    "('597_fri_c2_500_5', 500, 6),\n",
    "('598_fri_c0_1000_25', 1000, 26),\n",
    "('599_fri_c2_1000_5', 1000, 6),\n",
    "('601_fri_c1_250_5', 250, 6),\n",
    "('602_fri_c3_250_10', 250, 11),\n",
    "('603_fri_c0_250_50', 250, 51),\n",
    "('604_fri_c4_500_10', 500, 11),\n",
    "('605_fri_c2_250_25', 250, 26),\n",
    "('606_fri_c2_1000_10', 1000, 11),\n",
    "('607_fri_c4_1000_50', 1000, 51),\n",
    "('608_fri_c3_1000_10', 1000, 11),\n",
    "('609_fri_c0_1000_5', 1000, 6),\n",
    "('611_fri_c3_100_5', 100, 6),\n",
    "('612_fri_c1_1000_5', 1000, 6),\n",
    "('613_fri_c3_250_5', 250, 6),\n",
    "('615_fri_c4_250_10', 250, 11),\n",
    "('616_fri_c4_500_50', 500, 51),\n",
    "('617_fri_c3_500_5', 500, 6),\n",
    "('618_fri_c3_1000_50', 1000, 51),\n",
    "('620_fri_c1_1000_25', 1000, 26),\n",
    "('621_fri_c0_100_10', 100, 11),\n",
    "('622_fri_c2_1000_50', 1000, 51),\n",
    "('623_fri_c4_1000_10', 1000, 11),\n",
    "('624_fri_c0_100_5', 100, 6),\n",
    "('626_fri_c2_500_50', 500, 51),\n",
    "('627_fri_c2_500_10', 500, 11),\n",
    "('628_fri_c3_1000_5', 1000, 6),\n",
    "('631_fri_c1_500_5', 500, 6),\n",
    "('633_fri_c0_500_25', 500, 26),\n",
    "('634_fri_c2_100_10', 100, 11),\n",
    "('635_fri_c0_250_10', 250, 11),\n",
    "('637_fri_c1_500_50', 500, 51),\n",
    "('641_fri_c1_500_10', 500, 11),\n",
    "('643_fri_c2_500_25', 500, 26),\n",
    "('644_fri_c4_250_25', 250, 26),\n",
    "('645_fri_c3_500_50', 500, 51),\n",
    "('646_fri_c3_500_10', 500, 11),\n",
    "('647_fri_c1_250_10', 250, 11),\n",
    "('648_fri_c1_250_50', 250, 51),\n",
    "('649_fri_c0_500_5', 500, 6),\n",
    "('650_fri_c0_500_50', 500, 51),\n",
    "('651_fri_c0_100_25', 100, 26),\n",
    "('653_fri_c0_250_25', 250, 26),\n",
    "('654_fri_c0_500_10', 500, 11),\n",
    "('656_fri_c1_100_5', 100, 6),\n",
    "('657_fri_c2_250_10', 250, 11),\n",
    "('658_fri_c3_250_25', 250, 26),\n",
    "('659_sleuth_ex1714', 47, 8),\n",
    "('663_rabe_266', 120, 3),\n",
    "('665_sleuth_case2002', 147, 7),\n",
    "('666_rmftsa_ladata', 508, 11),\n",
    "('678_visualizing_environmental', 111, 4),\n",
    "('687_sleuth_ex1605', 62, 6),\n",
    "('690_visualizing_galaxy', 323, 5),\n",
    "('695_chatfield_4', 235, 13),\n",
    "('706_sleuth_case1202', 93, 7),\n",
    "('712_chscase_geyser1', 222, 3),\n",
    "('banana', 5300, 3),\n",
    "('titanic', 2201, 4),\n",
    "],\n",
    "columns=[\"name\",\"n\",\"fcount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_datasets = datasets[(datasets[\"n\"]>=1000)]\n",
    "len(chosen_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n</th>\n",
       "      <th>fcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1027_ESL</td>\n",
       "      <td>488</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028_SWD</td>\n",
       "      <td>1000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1029_LEV</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030_ERA</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1089_USCrime</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>695_chatfield_4</td>\n",
       "      <td>235</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>706_sleuth_case1202</td>\n",
       "      <td>93</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>712_chscase_geyser1</td>\n",
       "      <td>222</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>banana</td>\n",
       "      <td>5300</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>titanic</td>\n",
       "      <td>2201</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name     n  fcount\n",
       "0               1027_ESL   488       5\n",
       "1               1028_SWD  1000      11\n",
       "2               1029_LEV  1000       5\n",
       "3               1030_ERA  1000       5\n",
       "4           1089_USCrime    47      14\n",
       "..                   ...   ...     ...\n",
       "117      695_chatfield_4   235      13\n",
       "118  706_sleuth_case1202    93       7\n",
       "119  712_chscase_geyser1   222       3\n",
       "120               banana  5300       3\n",
       "121              titanic  2201       4\n",
       "\n",
       "[122 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30172.43442622951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.n.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.n.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmlb_data(name, randomseed, trainsize=250):\n",
    "    df = pmlb.fetch_data(name, local_cache_dir=\"pmlb_data\")\n",
    "    return train_test_split(df,train_size=trainsize, random_state=randomseed)\n",
    "\n",
    "from sklearn import svm, tree, linear_model, ensemble\n",
    "\n",
    "def X(df):\n",
    "    return df.iloc[:,:-1]\n",
    "\n",
    "def y(df):\n",
    "    return df.iloc[:,-1]\n",
    "\n",
    "def fit_and_r2_score(model, train, test):\n",
    "    model.fit(X(train), y(train))\n",
    "    return model.score(X(train), y(train)), model.score(X(test), y(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to the usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"dataset\", \"model\", \"randomseed\", \"train_r2\", \"test_r2\"])\n",
    "#results = pd.read_csv(\"results-cache-wip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>randomseed</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dataset, model, randomseed, train_r2, test_r2]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: dataset, dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: randomseed, dtype: int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.randomseed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = results[~(results[\"model\"].str.startswith(\"GradientBoostingRegressor(n_estimators=300)\"))].reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    linear_model.LinearRegression(),\n",
    "#    linear_model.Lasso(alpha=0.01, max_iter=100000),\n",
    "    linear_model.Lasso(alpha=0.05, max_iter=100000),\n",
    "#    linear_model.Lasso(alpha=0.10, max_iter=100000),\n",
    "\n",
    "#    tree.DecisionTreeRegressor(max_depth=1),\n",
    "#    tree.DecisionTreeRegressor(max_depth=2),\n",
    "#    tree.DecisionTreeRegressor(max_depth=4),\n",
    "#    tree.DecisionTreeRegressor(max_depth=6),\n",
    "    \n",
    "#    ensemble.RandomForestRegressor(n_estimators=400),\n",
    "    ensemble.RandomForestRegressor(n_estimators=200),\n",
    "#    ensemble.RandomForestRegressor(n_estimators=100),\n",
    "#    ensemble.RandomForestRegressor(n_estimators=50),\n",
    "\n",
    "#    ensemble.GradientBoostingRegressor(n_estimators=400),\n",
    "    ensemble.GradientBoostingRegressor(n_estimators=200),\n",
    "#    ensemble.GradientBoostingRegressor(n_estimators=100),\n",
    "#    ensemble.GradientBoostingRegressor(n_estimators=50),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_comparison(name, randomseed):    \n",
    "    global results\n",
    "\n",
    "    print(\"Seed %i, Dataset: %s\"%(randomseed,name), end=\"\")\n",
    "    train, test = get_pmlb_data(name, randomseed)\n",
    "    print(\" ... fetched\", end=\"\")\n",
    "    for m in models:\n",
    "        if ((results[\"dataset\"]==name) & (results[\"model\"]==str(m)) & (results[\"randomseed\"]==randomseed)).any():\n",
    "            # Skip if already run\n",
    "            continue\n",
    "        r2_train, r2_test = fit_and_r2_score(m, train, test)\n",
    "        new = pd.DataFrame([{\"dataset\": name, \"model\": str(m), \"randomseed\": randomseed, \"train_r2\": r2_train, \"test_r2\": r2_test}])\n",
    "        results = pd.concat([results, new])\n",
    "\n",
    "    print(\" ... and fitted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 3, Dataset: 1028_SWDSeed 2, Dataset: 1028_SWDSeed 0, Dataset: 1028_SWDSeed 1, Dataset: 1028_SWDSeed 4, Dataset: 1028_SWDSeed 0, Dataset: 1029_LEV ... fetched ... fetched ... fetched ... fetched ... fetched ... fetched ... and fitted ... and fitted\n",
      "\n",
      " ... and fittedSeed 1, Dataset: 1029_LEVSeed 2, Dataset: 1029_LEV ... and fitted\n",
      "\n",
      "Seed 3, Dataset: 1029_LEVSeed 4, Dataset: 1029_LEV ... fetched ... fetched ... fetched ... fetched ... and fitted\n",
      " ... and fittedSeed 0, Dataset: 1030_ERA\n",
      " ... and fittedSeed 1, Dataset: 1030_ERA\n",
      "\n",
      " ... and fittedSeed 2, Dataset: 1030_ERASeed 3, Dataset: 1030_ERA ... and fitted\n",
      " ... fetchedSeed 4, Dataset: 1030_ERA ... fetched ... fetched ... fetched ... and fitted ... fetched\n",
      " ... and fitted ... and fitted ... and fittedSeed 0, Dataset: 1191_BNG_pbc ... and fitted\n",
      "\n",
      "Seed 1, Dataset: 1191_BNG_pbcSeed 2, Dataset: 1191_BNG_pbc\n",
      " ... and fittedSeed 3, Dataset: 1191_BNG_pbc\n",
      "\n",
      "Seed 4, Dataset: 1191_BNG_pbcSeed 0, Dataset: 1193_BNG_lowbwt ... fetched ... and fitted\n",
      "Seed 1, Dataset: 1193_BNG_lowbwt ... fetched ... and fitted\n",
      "Seed 2, Dataset: 1193_BNG_lowbwt ... fetched ... and fitted\n",
      "Seed 3, Dataset: 1193_BNG_lowbwt ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1193_BNG_lowbwt ... fetched ... and fitted\n",
      "Seed 0, Dataset: 1196_BNG_pharynx ... fetched ... and fitted\n",
      "Seed 1, Dataset: 1196_BNG_pharynx ... fetched ... and fitted\n",
      "Seed 2, Dataset: 1196_BNG_pharynx ... fetched ... and fitted\n",
      "Seed 3, Dataset: 1196_BNG_pharynx ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1196_BNG_pharynx ... fetched ... and fitted ... fetched\n",
      "Seed 0, Dataset: 1199_BNG_echoMonths ... and fitted\n",
      "Seed 1, Dataset: 1199_BNG_echoMonths ... fetched ... and fitted\n",
      " ... fetchedSeed 2, Dataset: 1199_BNG_echoMonths ... and fitted\n",
      "Seed 3, Dataset: 1199_BNG_echoMonths ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1199_BNG_echoMonths ... fetched ... and fitted\n",
      "Seed 0, Dataset: 1201_BNG_breastTumor ... fetched ... and fitted\n",
      "Seed 1, Dataset: 1201_BNG_breastTumor ... fetched ... and fitted\n",
      "Seed 2, Dataset: 1201_BNG_breastTumor ... fetched ... and fitted\n",
      "Seed 3, Dataset: 1201_BNG_breastTumor ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1201_BNG_breastTumor ... fetched ... and fitted\n",
      "Seed 0, Dataset: 1203_BNG_pwLinear ... fetched ... and fitted\n",
      "Seed 1, Dataset: 1203_BNG_pwLinear ... fetched ... and fitted\n",
      "Seed 2, Dataset: 1203_BNG_pwLinear ... fetched ... and fitted\n",
      "Seed 3, Dataset: 1203_BNG_pwLinear ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1203_BNG_pwLinear ... fetched ... and fitted\n",
      "Seed 0, Dataset: 1595_poker ... fetched ... and fitted\n",
      "Seed 1, Dataset: 1595_poker ... fetched ... and fitted\n",
      "Seed 2, Dataset: 1595_poker ... fetched ... and fitted\n",
      "Seed 3, Dataset: 1595_poker ... fetched ... and fitted\n",
      "Seed 4, Dataset: 1595_poker ... fetched ... and fitted\n",
      "Seed 0, Dataset: 197_cpu_act ... fetched ... and fitted\n",
      "Seed 1, Dataset: 197_cpu_act ... fetched ... fetched ... and fitted\n",
      "Seed 2, Dataset: 197_cpu_act ... fetched ... and fitted\n",
      "Seed 3, Dataset: 197_cpu_act ... fetched ... fetched ... fetched ... and fitted\n",
      "Seed 4, Dataset: 197_cpu_act ... fetched ... fetched ... and fitted\n",
      "Seed 0, Dataset: 201_pol ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 201_pol ... and fitted\n",
      "Seed 2, Dataset: 201_pol ... fetched ... fetched ... and fitted\n",
      "Seed 3, Dataset: 201_pol ... fetched ... and fitted ... and fitted\n",
      "\n",
      "Seed 4, Dataset: 201_polSeed 0, Dataset: 215_2dplanes ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 215_2dplanes ... fetched ... and fitted\n",
      "Seed 2, Dataset: 215_2dplanes ... fetched ... and fitted\n",
      "Seed 3, Dataset: 215_2dplanes ... fetched ... and fitted\n",
      "Seed 4, Dataset: 215_2dplanes ... fetched ... and fitted\n",
      "Seed 0, Dataset: 218_house_8L ... fetched ... and fitted\n",
      "Seed 1, Dataset: 218_house_8L ... fetched ... and fitted\n",
      "Seed 2, Dataset: 218_house_8L ... fetched ... and fitted\n",
      "Seed 3, Dataset: 218_house_8L ... fetched ... and fitted\n",
      "Seed 4, Dataset: 218_house_8L ... fetched ... and fitted\n",
      "Seed 0, Dataset: 225_puma8NH ... fetched ... and fitted\n",
      "Seed 1, Dataset: 225_puma8NH ... fetched ... and fitted\n",
      "Seed 2, Dataset: 225_puma8NH ... and fitted\n",
      "Seed 3, Dataset: 225_puma8NH ... fetched ... fetched ... and fitted\n",
      "Seed 4, Dataset: 225_puma8NH ... fetched ... and fitted\n",
      "Seed 0, Dataset: 227_cpu_small ... fetched ... and fitted\n",
      "Seed 1, Dataset: 227_cpu_small ... and fitted\n",
      "Seed 2, Dataset: 227_cpu_small ... fetched ... fetched ... and fitted\n",
      "Seed 3, Dataset: 227_cpu_small ... fetched ... and fitted\n",
      "Seed 4, Dataset: 227_cpu_small ... fetched ... and fitted\n",
      "Seed 0, Dataset: 294_satellite_image ... fetched ... and fitted\n",
      "Seed 1, Dataset: 294_satellite_image ... fetched ... and fitted\n",
      "Seed 2, Dataset: 294_satellite_image ... fetched ... and fitted\n",
      "Seed 3, Dataset: 294_satellite_image ... fetched ... and fitted\n",
      "Seed 4, Dataset: 294_satellite_image ... fetched ... and fitted\n",
      "Seed 0, Dataset: 344_mv ... fetched ... and fitted\n",
      "Seed 1, Dataset: 344_mv ... fetched ... and fitted\n",
      "Seed 2, Dataset: 344_mv ... fetched ... and fitted\n",
      "Seed 3, Dataset: 344_mv ... fetched ... and fitted\n",
      "Seed 4, Dataset: 344_mv ... and fitted\n",
      "Seed 0, Dataset: 4544_GeographicalOriginalofMusic ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 4544_GeographicalOriginalofMusic ... fetched ... and fitted\n",
      "Seed 2, Dataset: 4544_GeographicalOriginalofMusic ... fetched ... and fitted\n",
      "Seed 3, Dataset: 4544_GeographicalOriginalofMusic ... fetched ... and fitted\n",
      "Seed 4, Dataset: 4544_GeographicalOriginalofMusic ... fetched ... and fitted\n",
      "Seed 0, Dataset: 503_wind ... fetched ... and fitted\n",
      "Seed 1, Dataset: 503_wind ... fetched ... and fitted\n",
      "Seed 2, Dataset: 503_wind ... fetched ... and fitted\n",
      "Seed 3, Dataset: 503_wind ... fetched ... and fitted\n",
      "Seed 4, Dataset: 503_wind ... fetched ... and fitted\n",
      "Seed 0, Dataset: 529_pollen ... fetched ... and fitted\n",
      "Seed 1, Dataset: 529_pollen ... fetched ... and fitted\n",
      "Seed 2, Dataset: 529_pollen ... fetched ... and fitted\n",
      "Seed 3, Dataset: 529_pollen ... fetched ... and fitted\n",
      "Seed 4, Dataset: 529_pollen ... fetched ... and fitted\n",
      "Seed 0, Dataset: 537_houses ... fetched ... and fitted\n",
      "Seed 1, Dataset: 537_houses ... fetched ... and fitted\n",
      "Seed 2, Dataset: 537_houses ... fetched ... and fitted\n",
      "Seed 3, Dataset: 537_houses ... and fitted\n",
      "Seed 4, Dataset: 537_houses ... and fitted\n",
      "Seed 0, Dataset: 562_cpu_small ... fetched ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 562_cpu_small ... and fitted\n",
      "Seed 2, Dataset: 562_cpu_small ... fetched ... fetched ... and fitted\n",
      "Seed 3, Dataset: 562_cpu_small ... fetched ... and fitted\n",
      "Seed 4, Dataset: 562_cpu_small ... fetched ... and fitted\n",
      "Seed 0, Dataset: 564_fried ... and fitted\n",
      "Seed 1, Dataset: 564_fried ... fetched ... fetched ... and fitted\n",
      "Seed 2, Dataset: 564_fried ... and fitted\n",
      "Seed 3, Dataset: 564_fried ... and fitted ... and fitted\n",
      "\n",
      "Seed 4, Dataset: 564_friedSeed 0, Dataset: 573_cpu_act ... fetched ... fetched ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 573_cpu_act ... fetched ... and fitted\n",
      "Seed 2, Dataset: 573_cpu_act ... fetched ... and fitted\n",
      "Seed 3, Dataset: 573_cpu_act ... fetched ... and fitted\n",
      "Seed 4, Dataset: 573_cpu_act ... and fitted ... fetched\n",
      "Seed 0, Dataset: 574_house_16H ... fetched ... and fitted\n",
      "Seed 1, Dataset: 574_house_16H ... and fitted\n",
      "Seed 2, Dataset: 574_house_16H ... and fitted\n",
      "Seed 3, Dataset: 574_house_16H ... fetched ... fetched ... fetched ... and fitted\n",
      "Seed 4, Dataset: 574_house_16H ... fetched ... and fitted\n",
      "Seed 0, Dataset: 583_fri_c1_1000_50 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 583_fri_c1_1000_50 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 583_fri_c1_1000_50 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 583_fri_c1_1000_50 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 583_fri_c1_1000_50 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 586_fri_c3_1000_25 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 586_fri_c3_1000_25 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 586_fri_c3_1000_25 ... and fitted\n",
      "Seed 3, Dataset: 586_fri_c3_1000_25 ... fetched ... fetched ... and fitted\n",
      "Seed 4, Dataset: 586_fri_c3_1000_25 ... and fitted\n",
      "Seed 0, Dataset: 588_fri_c4_1000_100 ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 588_fri_c4_1000_100 ... fetched ... and fitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2, Dataset: 588_fri_c4_1000_100 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 588_fri_c4_1000_100 ... and fitted\n",
      "Seed 4, Dataset: 588_fri_c4_1000_100 ... fetched ... fetched ... and fitted\n",
      "Seed 0, Dataset: 589_fri_c2_1000_25 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 589_fri_c2_1000_25 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 589_fri_c2_1000_25 ... fetched ... and fitted\n",
      " ... and fittedSeed 3, Dataset: 589_fri_c2_1000_25\n",
      " ... fetchedSeed 4, Dataset: 589_fri_c2_1000_25 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 590_fri_c0_1000_50 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 590_fri_c0_1000_50 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 590_fri_c0_1000_50 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 590_fri_c0_1000_50 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 590_fri_c0_1000_50 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 592_fri_c4_1000_25 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 592_fri_c4_1000_25 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 592_fri_c4_1000_25 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 592_fri_c4_1000_25 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 592_fri_c4_1000_25 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 593_fri_c1_1000_10 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 593_fri_c1_1000_10 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 593_fri_c1_1000_10 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 593_fri_c1_1000_10 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 593_fri_c1_1000_10 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 595_fri_c0_1000_10 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 595_fri_c0_1000_10 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 595_fri_c0_1000_10 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 595_fri_c0_1000_10 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 595_fri_c0_1000_10 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 598_fri_c0_1000_25 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 598_fri_c0_1000_25 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 598_fri_c0_1000_25 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 598_fri_c0_1000_25 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 598_fri_c0_1000_25 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 599_fri_c2_1000_5 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 599_fri_c2_1000_5 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 599_fri_c2_1000_5 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 599_fri_c2_1000_5 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 599_fri_c2_1000_5 ... and fitted\n",
      "Seed 0, Dataset: 606_fri_c2_1000_10 ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: 606_fri_c2_1000_10 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 606_fri_c2_1000_10 ... and fitted\n",
      "Seed 3, Dataset: 606_fri_c2_1000_10 ... fetched ... fetched ... and fitted\n",
      "Seed 4, Dataset: 606_fri_c2_1000_10 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 607_fri_c4_1000_50 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 607_fri_c4_1000_50 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 607_fri_c4_1000_50 ... and fitted\n",
      "Seed 3, Dataset: 607_fri_c4_1000_50 ... and fitted\n",
      "Seed 4, Dataset: 607_fri_c4_1000_50 ... fetched ... fetched ... fetched ... and fitted\n",
      "Seed 0, Dataset: 608_fri_c3_1000_10 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 608_fri_c3_1000_10 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 608_fri_c3_1000_10 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 608_fri_c3_1000_10 ... and fitted ... fetched\n",
      "Seed 4, Dataset: 608_fri_c3_1000_10 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 609_fri_c0_1000_5 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 609_fri_c0_1000_5 ... and fitted\n",
      "Seed 2, Dataset: 609_fri_c0_1000_5 ... fetched ... fetched ... and fitted\n",
      "Seed 3, Dataset: 609_fri_c0_1000_5 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 609_fri_c0_1000_5 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 612_fri_c1_1000_5 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 612_fri_c1_1000_5 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 612_fri_c1_1000_5 ... and fitted ... fetched\n",
      "Seed 3, Dataset: 612_fri_c1_1000_5 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 612_fri_c1_1000_5 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 618_fri_c3_1000_50 ... and fitted\n",
      "Seed 1, Dataset: 618_fri_c3_1000_50 ... fetched ... fetched ... and fitted\n",
      "Seed 2, Dataset: 618_fri_c3_1000_50 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 618_fri_c3_1000_50 ... and fitted\n",
      "Seed 4, Dataset: 618_fri_c3_1000_50 ... fetched ... fetched ... and fitted\n",
      "Seed 0, Dataset: 620_fri_c1_1000_25 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 620_fri_c1_1000_25 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 620_fri_c1_1000_25 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 620_fri_c1_1000_25 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 620_fri_c1_1000_25 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 622_fri_c2_1000_50 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 622_fri_c2_1000_50 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 622_fri_c2_1000_50 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 622_fri_c2_1000_50 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 622_fri_c2_1000_50 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 623_fri_c4_1000_10 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 623_fri_c4_1000_10 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 623_fri_c4_1000_10 ... fetched ... and fitted\n",
      "Seed 3, Dataset: 623_fri_c4_1000_10 ... fetched ... and fitted\n",
      "Seed 4, Dataset: 623_fri_c4_1000_10 ... fetched ... and fitted\n",
      "Seed 0, Dataset: 628_fri_c3_1000_5 ... fetched ... and fitted\n",
      "Seed 1, Dataset: 628_fri_c3_1000_5 ... fetched ... and fitted\n",
      "Seed 2, Dataset: 628_fri_c3_1000_5 ... fetched ... and fitted\n",
      " ... and fittedSeed 3, Dataset: 628_fri_c3_1000_5\n",
      "Seed 4, Dataset: 628_fri_c3_1000_5 ... fetched ... fetched ... and fitted\n",
      "Seed 0, Dataset: banana ... fetched ... and fitted\n",
      "Seed 1, Dataset: banana ... fetched ... and fitted\n",
      "Seed 2, Dataset: banana ... fetched ... and fitted\n",
      "Seed 3, Dataset: banana ... fetched ... and fitted\n",
      "Seed 4, Dataset: banana ... and fitted\n",
      "Seed 0, Dataset: titanic ... fetched ... fetched ... and fitted\n",
      "Seed 1, Dataset: titanic ... fetched ... and fitted\n",
      "Seed 2, Dataset: titanic ... fetched ... and fitted\n",
      "Seed 3, Dataset: titanic ... fetched ... and fitted\n",
      "Seed 4, Dataset: titanic ... fetched ... and fitted\n",
      " ... and fitted\n",
      " ... and fitted\n",
      " ... and fitted\n",
      " ... and fitted\n",
      " ... and fitted\n"
     ]
    }
   ],
   "source": [
    "# Create a pool of workers to run in parallel\n",
    "with multiprocessing.Pool(processes=6) as pool:\n",
    "    jobargs = [(name, seed) for name in chosen_datasets[\"name\"] for seed in range(0,5) ]\n",
    "\n",
    "    # Map the function to the inputs, distributing the work across multiple processes\n",
    "    r = pool.starmap(fit_comparison, jobargs, chunksize=1)\n",
    "\n",
    "\n",
    "\n",
    "#for name in chosen_datasets[\"name\"]:\n",
    "#    for randomseed in range(0,5):\n",
    "#        fit_comparison(name, randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a qgraph for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_qmodel(edges, criterion, randomseed):\n",
    "    global results\n",
    "\n",
    "    key = f\"QG-{edges}-{criterion}\"\n",
    "\n",
    "    for name in chosen_datasets[\"name\"]:    \n",
    "        if ((results[\"dataset\"]==name) & (results[\"model\"]==key) & (results[\"randomseed\"]==randomseed)).any():\n",
    "            # Skip if already run\n",
    "            continue\n",
    "\n",
    "        train, test = get_pmlb_data(name, randomseed)\n",
    "        ql.reset(randomseed)\n",
    "        \n",
    "        qg = ql.get_regressor(train.columns, train.columns[-1]).filter(feyn.filters.MaxEdges(edges))\n",
    "    \n",
    "        for _ in range(100):\n",
    "            qg.fit(train, threads=10, criterion=criterion)\n",
    "            print(key, randomseed, name)\n",
    "            print(\"Train:\\t\", qg[0].r2_score(train), \"\\nTest:\\t\", qg[0].r2_score(test))\n",
    "\n",
    "            ql.update(qg.best())\n",
    "\n",
    "        for _ in range(1000):\n",
    "            qg[0].fit(train)\n",
    "\n",
    "        results = results.append({\"dataset\": name, \"model\": key, \"randomseed\": randomseed, \"train_r2\": qg[0].r2_score(train), \"test_r2\": qg[0].r2_score(test)}, ignore_index=True)\n",
    "\n",
    "        time.sleep(6) # Protect my poor cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit all QGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for randomseed in range(0,5):\n",
    "    fit_qgraph(11, \"bic\", randomseed=randomseed)\n",
    "    fit_qgraph(11, \"aic\", randomseed=randomseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_among(models=None, datasets=None):\n",
    "    if models is None:\n",
    "        models = results[\"model\"].unique()\n",
    "    if datasets is None:\n",
    "        datasets = results[\"dataset\"].unique()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        subset = results[(results[\"dataset\"] == dataset) & (results[\"test_r2\"]>-1)]\n",
    "        seeds = subset[\"randomseed\"].unique()\n",
    "        for seed in seeds:\n",
    "            subsubset = subset[subset[\"randomseed\"]==seed].sort_values(by=\"test_r2\")\n",
    "            subsubset.plot.barh(x=\"model\", y=[\"test_r2\",\"train_r2\"], title=dataset+\" \"+str(seed),figsize=(8,6))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_among(None,[\"598_fri_c0_1000_25\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for name in chosen_datasets[\"name\"]:\n",
    "#    for seed in range(0,5):\n",
    "#        subres = results[(results[\"dataset\"] == name) & (results[\"randomseed\"]==seed) & (results[\"test_r2\"]>-1)].sort_values(by=\"test_r2\")\n",
    "#        if len(subres):\n",
    "#            subres.plot.barh(x=\"model\", y=[\"test_r2\",\"train_r2\"], title=name,figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f\"results-cache-wip.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_among(models=None, rankpositions = None):\n",
    "    if models is None:\n",
    "        models = results[\"model\"].unique()\n",
    "\n",
    "    if rankpositions is None:\n",
    "        rankpositions = len(models)-1\n",
    "\n",
    "    # Only consider results for the chosen models\n",
    "    res = results[results[\"model\"].isin(models)].sort_values(by=\"test_r2\", ascending=False)\n",
    "    \n",
    "    points = {m: 0 for m in models}\n",
    "    \n",
    "    for name in res[\"dataset\"].unique(): # For each dataset\n",
    "        for seed in res[\"randomseed\"].unique(): # For each seed\n",
    "            subset = res[(res[\"dataset\"]==name) & (res[\"randomseed\"]==seed)]\n",
    "            if len(subset):\n",
    "                for rank in range(rankpositions): # For each rank position\n",
    "                    m = subset.iloc[rank].model\n",
    "                    r2 = subset.iloc[rank].test_r2\n",
    "                    points[m] += rankpositions - rank\n",
    "    return pd.DataFrame(points.items(), columns=[\"model\", \"points\"]).sort_values(by=\"points\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_among(rankpositions=1).plot.barh(x=\"model\", y=\"points\", label=\"First places\", figsize=(8,6), xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_among(rankpositions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_among(rankpositions=5).plot.barh(x=\"model\", y=\"points\", label=\"Points\", figsize=(8,6), xlabel=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_among(rankpositions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_among(\n",
    "    models=[\"QG-11-bic\", \n",
    "            \"GradientBoostingRegressor(n_estimators=400)\", \n",
    "            \"RandomForestRegressor(n_estimators=400)\",\n",
    "            \"Lasso(alpha=0.01, max_iter=100000)\",\n",
    "            \"DecisionTreeRegressor(max_depth=1)\"\n",
    "           ],\n",
    "    rankpositions=5\n",
    ").plot.barh(x=\"model\", y=\"points\", label=\"First places\", figsize=(8,1.6), xlabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings to LaTeX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_scoring = rank_among(rankpositions=1)\n",
    "second_scoring = rank_among(rankpositions=5)\n",
    "best_scoring = rank_among(\n",
    "    models=[\"QG-11-bic\", \n",
    "            \"Lasso(alpha=0.1, max_iter=100000)\",\n",
    "            \"GradientBoostingRegressor(n_estimators=400)\", \n",
    "            \"RandomForestRegressor(n_estimators=400)\",\n",
    "            \"DecisionTreeRegressor(max_depth=2)\"\n",
    "           ],\n",
    "    rankpositions=1\n",
    ")\n",
    "best_weighted = rank_among(\n",
    "    models=[\"QG-11-bic\", \n",
    "            \"Lasso(alpha=0.1, max_iter=100000)\",\n",
    "            \"GradientBoostingRegressor(n_estimators=400)\", \n",
    "            \"RandomForestRegressor(n_estimators=400)\",\n",
    "            \"DecisionTreeRegressor(max_depth=2)\"\n",
    "           ],\n",
    "    rankpositions=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_scoring[\"First places\"] = first_scoring.points\n",
    "second_scoring[\"Weighted scoring\"] = second_scoring.points\n",
    "best_scoring[\"First places for best\"] = best_scoring.points\n",
    "best_weighted[\"Weighted scoring for best\"] = best_weighted.points\n",
    "\n",
    "\n",
    "first_scoring.index = first_scoring.model\n",
    "second_scoring.index = first_scoring.model\n",
    "best_scoring.index = best_scoring.model\n",
    "best_weighted.index = best_weighted.model\n",
    "\n",
    "first_scoring = first_scoring.drop([\"model\", \"points\"], axis=1)\n",
    "second_scoring = second_scoring.drop([\"model\", \"points\"], axis=1)\n",
    "best_scoring = best_scoring.drop([\"model\", \"points\"], axis=1)\n",
    "best_weighted = best_weighted.drop([\"model\", \"points\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_results = first_scoring.join(second_scoring).join(best_scoring).join(best_weighted).sort_values(by=\"First places\", ascending=False)\n",
    "latex_results[\"First places for best\"] = latex_results[\"First places for best\"].replace(np.nan, 0).astype(int)\n",
    "latex_results[\"First places for best\"] = latex_results[\"First places for best\"].replace(0, \"\")\n",
    "latex_results[\"Weighted scoring for best\"] = latex_results[\"Weighted scoring for best\"].replace(np.nan, 0).astype(int)\n",
    "latex_results[\"Weighted scoring for best\"] = latex_results[\"Weighted scoring for best\"].replace(0, \"\")\n",
    "print(latex_results.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex formatting for the figures that we include in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# Use Abzu colors\n",
    "\n",
    "abzu_rgba = {\n",
    "    'Dark Jungle Green': (0.11764705882352941, 0.11764705882352941, 0.11764705882352941, 1.0),\n",
    "    'Golden Yellow': (1.0, 1.0, 0.0392156862745098, 1.0),\n",
    "    'Guppie Green': (0.0, 0.9411764705882353, 0.5098039215686274, 1.0),\n",
    "    'Hot Magenta': (1.0, 0.11764705882352941, 0.7843137254901961, 1.0),\n",
    "    'Majorelle Blue': (0.27450980392156865, 0.27450980392156865, 0.9019607843137255, 1.0),\n",
    "    'Robin Egg Blue': (0.0, 0.7843137254901961, 0.7843137254901961, 1.0),\n",
    "    'Safety Orange': (1.0, 0.39215686274509803, 0.0392156862745098, 1.0),\n",
    "    'Spiro Disco Ball': (0.0392156862745098, 0.7058823529411765, 0.9803921568627451, 1.0)\n",
    "}\n",
    "\n",
    "def get_abzu_cmap(cname):\n",
    "    r, g, b, _ = abzu_rgba[cname]\n",
    "    ret = []\n",
    "    for alphaval in np.linspace(0.03, 0.9, 256):\n",
    "        ret.append([r, g, b, alphaval])\n",
    "    return ListedColormap(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rc('xtick',labelsize=10)\n",
    "mpl.rc('ytick',labelsize=10)\n",
    "\n",
    "mpl.rc('axes',labelsize=15)\n",
    "mpl.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "mpl.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "chosen_datasets.plot.scatter(\n",
    "    x=\"n\",\n",
    "    y=\"fcount\",\n",
    "    loglog=True,\n",
    "    ylabel=\"Number of features\",\n",
    "    xlabel=\"Number of observations\",\n",
    "    color=abzu_rgba['Majorelle Blue'],\n",
    "    s=40,\n",
    "    ax=ax\n",
    ")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/home/jaan/devel/regressionArticle/jmlr/manuscript/figures/dataset_summary.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for a results figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    \"QG-11-bic\", \n",
    "    \"GradientBoostingRegressor(n_estimators=400)\", \n",
    "    \"RandomForestRegressor(n_estimators=400)\",\n",
    "    \"Lasso(alpha=0.01, max_iter=100000)\",\n",
    "    \"DecisionTreeRegressor(max_depth=2)\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = results[results.model.isin(best_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {'train_r2': 'mean', 'test_r2': 'mean'}\n",
    "sr_results = best_results[best_results.model.apply(lambda x: 'QG' in x)].groupby('dataset').agg(agg_dict)\n",
    "dt_results = best_results[best_results.model.apply(lambda x: 'DecisionTree' in x)].groupby('dataset').agg(agg_dict)\n",
    "gb_results = best_results[best_results.model.apply(lambda x: 'GradientBoosting' in x)].groupby('dataset').agg(agg_dict)\n",
    "rf_results = best_results[best_results.model.apply(lambda x: 'RandomForest' in x)].groupby('dataset').agg(agg_dict)\n",
    "lr_results = best_results[best_results.model.apply(lambda x: 'Lasso' in x)].groupby('dataset').agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{sr_results=}'.split('=')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sr_results.test_r2 < 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative test R^2 results\n",
    "for name, method_df in {\"sr\": sr_results, \"dt\": dt_results, \"gb\": gb_results, \"rf\": rf_results, \"lr\": lr_results}.items():\n",
    "    print(name, (method_df.test_r2 < 0).sum())\n",
    "    \n",
    "del name\n",
    "del method_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_cloud(df):\n",
    "    return np.vstack([df.train_r2.ravel(), df.test_r2.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr, Yr = np.mgrid[0:1:0.01, 0:1:0.01]\n",
    "all_positions = np.vstack([Xr.ravel(), Yr.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(abzu_rgba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "for sax in ax.flatten():\n",
    "    sax.set_ylim(0,1)\n",
    "    sax.set_xlim(0,1)\n",
    "    #sax.set_xlabel(r'Train $R^2$')\n",
    "    #sax.set_ylabel(r'Test $R^2$')\n",
    "    \n",
    "fig.text(0.5, 0.04, r'Training $R^2$', ha='center', size=20)\n",
    "fig.text(0.04, 0.5, r'Validation $R^2$', va='center', rotation='vertical', size=20)\n",
    "\n",
    "scatter_density = np.reshape(gaussian_kde(df_to_cloud(sr_results))(all_positions).T, Xr.shape)\n",
    "ax[0,0].imshow(np.rot90(scatter_density), extent=[0,1,0,1], alpha=.5, cmap=get_abzu_cmap('Spiro Disco Ball'))\n",
    "ax[0,0].plot(np.linspace(0,1,100), np.linspace(0,1,100), ls=\"--\", color=\"black\", alpha=0.4)\n",
    "ax[0,0].scatter(sr_results.train_r2, sr_results.test_r2, color=abzu_rgba['Spiro Disco Ball'])\n",
    "ax[0,0].set_title(r\"\\textbf{(A)} QLattice with BIC\")\n",
    "\n",
    "scatter_density = np.reshape(gaussian_kde(df_to_cloud(lr_results))(all_positions).T, Xr.shape)\n",
    "ax[0,1].imshow(np.rot90(scatter_density), extent=[0,1,0,1], alpha=.5, cmap=get_abzu_cmap('Guppie Green'))\n",
    "ax[0,1].plot(np.linspace(0,1,100), np.linspace(0,1,100), ls=\"--\", color=\"black\", alpha=0.4)\n",
    "ax[0,1].scatter(lr_results.train_r2, lr_results.test_r2, color=abzu_rgba['Guppie Green'])\n",
    "ax[0,1].set_title(r\"\\textbf{(B)} Lasso Regression ($\\alpha=0.1$)\")\n",
    "\n",
    "scatter_density = np.reshape(gaussian_kde(df_to_cloud(dt_results))(all_positions).T, Xr.shape)\n",
    "ax[0,2].imshow(np.rot90(scatter_density), extent=[0,1,0,1], alpha=.5, cmap=get_abzu_cmap('Safety Orange'))\n",
    "ax[0,2].plot(np.linspace(0,1,100), np.linspace(0,1,100), ls=\"--\", color=\"black\", alpha=0.4)\n",
    "ax[0,2].scatter(dt_results.train_r2, dt_results.test_r2, color=abzu_rgba['Safety Orange'])\n",
    "ax[0,2].set_title(r\"\\textbf{(C)} Decision Tree (Max depth 2)\")\n",
    "\n",
    "scatter_density = np.reshape(gaussian_kde(df_to_cloud(gb_results))(all_positions).T, Xr.shape)\n",
    "ax[1,0].imshow(np.rot90(scatter_density), extent=[0,1,0,1], alpha=.5, cmap=get_abzu_cmap('Hot Magenta'))\n",
    "ax[1,0].plot(np.linspace(0,1,100), np.linspace(0,1,100), ls=\"--\", color=\"black\", alpha=0.4)\n",
    "ax[1,0].scatter(gb_results.train_r2, gb_results.test_r2, color=abzu_rgba['Hot Magenta'])\n",
    "ax[1,0].set_title(r\"\\textbf{(D)} Gradient Boosting (400)\")\n",
    "\n",
    "scatter_density = np.reshape(gaussian_kde(df_to_cloud(rf_results))(all_positions).T, Xr.shape)\n",
    "ax[1,1].imshow(np.rot90(scatter_density), extent=[0,1,0,1], alpha=.5, cmap=get_abzu_cmap('Majorelle Blue'))\n",
    "ax[1,1].plot(np.linspace(0,1,100), np.linspace(0,1,100), ls=\"--\", color=\"black\", alpha=0.4)\n",
    "ax[1,1].scatter(rf_results.train_r2, rf_results.test_r2, color=abzu_rgba['Majorelle Blue'])\n",
    "ax[1,1].set_title(r\"\\textbf{(E)} Random Forest (400)\")\n",
    "\n",
    "ax[1,2].set_axis_off()\n",
    "\n",
    "#plt.savefig(\"/home/jaan/devel/regressionArticle/jmlr/manuscript/figures/scatters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
